{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9435733,"sourceType":"datasetVersion","datasetId":5733239},{"sourceId":9435879,"sourceType":"datasetVersion","datasetId":5733337},{"sourceId":10130014,"sourceType":"datasetVersion","datasetId":6251681}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets\n!pip install penman\n!wget -O amr3.0.tar.gz https://amr.isi.edu/download/amr-bank-3.0.txt\n!mkdir amr_data\n!tar -xvzf amr3.0.tar.gz -C amr_data\n!pip install transformers datasets torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-12-08T11:35:23.291760Z","iopub.execute_input":"2024-12-08T11:35:23.292429Z","iopub.status.idle":"2024-12-08T11:35:51.816730Z","shell.execute_reply.started":"2024-12-08T11:35:23.292396Z","shell.execute_reply":"2024-12-08T11:35:51.815603Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.1.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.26.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.6.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: penman in /opt/conda/lib/python3.10/site-packages (1.3.1)\n--2024-12-08 11:35:41--  https://amr.isi.edu/download/amr-bank-3.0.txt\nResolving amr.isi.edu (amr.isi.edu)... failed: Name or service not known.\nwget: unable to resolve host address 'amr.isi.edu'\nmkdir: cannot create directory 'amr_data': File exists\n\ngzip: stdin: unexpected end of file\ntar: Child returned status 1\ntar: Error is not recoverable: exiting now\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.1.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.26.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"### AMR dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load AMR dataset\ndataset = load_dataset(\"tverous/anli-amr\", split=\"train\")\n\n# View the data\nprint(dataset[0])\nprint(dataset.shape)","metadata":{"execution":{"iopub.status.busy":"2024-12-08T11:35:51.818588Z","iopub.execute_input":"2024-12-08T11:35:51.818894Z","iopub.status.idle":"2024-12-08T11:35:54.295123Z","shell.execute_reply.started":"2024-12-08T11:35:51.818867Z","shell.execute_reply":"2024-12-08T11:35:54.294253Z"},"trusted":true},"outputs":[{"name":"stdout","text":"{'uid': '2093cfb3-a15f-4282-81e3-0cb793ffd0d7', 'premise': 'TOKYO, Dec 18 (Reuters) - Japanâ€™s Shionogi & Co said on Tuesday that it has applied to health regulators in the United States, Canada and Europe for approval of its HIV drug Dolutegravir. Shionogi developed Dolutegravir with a Viiv Healthcare, an AIDS drug joint venture between GlaxoSmithKline and Pfizer, in exchange for its rights to the drug.', 'hypothesis': 'The article was written on December 18th.', 'label': 0, 'reason': 'TOKYO, Dec 18 (Reuters) is when the article was written as it states in the first words of the sentence', 'claim_cleaned_amr': '( z0 write :ARG1 ( z1 article ) :time ( z2 date-entity :day 18 :month 12 ) )', 'amr_penman': '(z0 / write-01\\n    :ARG1 (z1 / article)\\n    :time (z2 / date-entity\\n              :day 18\\n              :month 12))', 'amr_tokens': ['The', 'article', 'was', 'written', 'on', 'December', '18th', '.'], 'amr_nodes': \"{'z1': 'article', 'z0': 'write-01', 'z2': 'date-entity', '0': '12', '1': '18'}\", 'amr_alignments': \"{'z1': [1], 'z0': [3], 'z2': [5], '0': [5], '1': [6]}\", 'amr_edges': [['z0', ':ARG1', 'z1'], ['z0', ':time', 'z2'], ['z2', ':month', '0'], ['z2', ':day', '1']]}\n(100459, 11)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from datasets import load_dataset\nimport pandas as pd\n\n\n\n# Function to extract AMR graph and text\ndef extract_amr_and_text(data):\n    amr_text_pairs = []\n    for row in data:\n        amr_graph = row.get(\"amr_penman\", None)\n        text = row.get(\"hypothesis\", None)\n        if amr_graph and text:\n            amr_text_pairs.append({\"amr_graph\": amr_graph, \"text\": text})\n    return amr_text_pairs\n\n# Extract AMR graphs and texts for all rows\namr_text_pairs = extract_amr_and_text(dataset)\n\namrs = []\ntexts = []\n\nfor i in range(100459):\n    amrs.append(amr_text_pairs[i]['amr_graph'])\n    texts.append(amr_text_pairs[i]['text'])\n\n# Creating DataFrame with 'amr_graph' and 'text' columns\ndata_amr = pd.DataFrame({\n    'amr_graph': amrs,\n    'text': texts\n})\n\ndata_amr.head()  # Displaying the first few rows to verify","metadata":{"execution":{"iopub.status.busy":"2024-12-08T11:35:54.296092Z","iopub.execute_input":"2024-12-08T11:35:54.296352Z","iopub.status.idle":"2024-12-08T11:36:06.706650Z","shell.execute_reply.started":"2024-12-08T11:35:54.296327Z","shell.execute_reply":"2024-12-08T11:36:06.705738Z"},"trusted":true},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                           amr_graph  \\\n0  (z0 / write-01\\n    :ARG1 (z1 / article)\\n    ...   \n1  (z0 / urge-01\\n    :ARG0 (z1 / person\\n       ...   \n2  (z0 / and\\n    :op1 (z1 / beat-03\\n           ...   \n3  (z0 / close-01\\n    :ARG1 (z1 / road)\\n    :du...   \n4    (z0 / advise-01\\n    :ARG2 (z1 / slow-down-03))   \n\n                                                text  \n0          The article was written on December 18th.  \n1  Gillum was on TV urging residents to stay out ...  \n2  Carlton beat Melbourne in 2016 and will attemp...  \n3  The road was closed for more than two hours af...  \n4                         Its advisible to slow down  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>amr_graph</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(z0 / write-01\\n    :ARG1 (z1 / article)\\n    ...</td>\n      <td>The article was written on December 18th.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(z0 / urge-01\\n    :ARG0 (z1 / person\\n       ...</td>\n      <td>Gillum was on TV urging residents to stay out ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>(z0 / and\\n    :op1 (z1 / beat-03\\n           ...</td>\n      <td>Carlton beat Melbourne in 2016 and will attemp...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>(z0 / close-01\\n    :ARG1 (z1 / road)\\n    :du...</td>\n      <td>The road was closed for more than two hours af...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(z0 / advise-01\\n    :ARG2 (z1 / slow-down-03))</td>\n      <td>Its advisible to slow down</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"### Transformer Finetuning","metadata":{}},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Convert the DataFrame to a Hugging Face dataset\ndataset = Dataset.from_pandas(data_amr)\n\n# Select a smaller subset if needed\nsmall_dataset = dataset.select([i for i in range(25000)])\n\n# Define prompt and answer templates\nprompt_template = \"\"\"Translate from Graph to Text- Instruction: {instruction}\\n Response:\"\"\"\nanswer_template = \"\"\"{response}\"\"\"\n\n# Define function to add keys in the dictionary for prompt, answer, and combined text\ndef _add_text(rec):\n    instruction = rec[\"amr_graph\"]  # Use amr_graph as instruction\n    response = rec[\"text\"]  # Use text as response\n    \n    # Check if both exist; raise error if not\n    if not instruction:\n        raise ValueError(f\"Expected an instruction (amr_graph) in: {rec}\")\n    if not response:\n        raise ValueError(f\"Expected a response (text) in: {rec}\")\n    \n    # Create prompt, answer, and combined text\n    rec[\"prompt\"] = prompt_template.format(instruction=instruction)\n    rec[\"answer\"] = answer_template.format(response=response)\n    rec[\"text\"] = rec[\"prompt\"] + rec[\"answer\"]\n    return rec\n\n# Apply the function to the dataset\nsmall_dataset = small_dataset.map(_add_text)\n\n# Print the first item to check\nprint(small_dataset[0])\n","metadata":{"execution":{"iopub.status.busy":"2024-12-08T11:36:06.709161Z","iopub.execute_input":"2024-12-08T11:36:06.710033Z","iopub.status.idle":"2024-12-08T11:36:08.297911Z","shell.execute_reply.started":"2024-12-08T11:36:06.709992Z","shell.execute_reply":"2024-12-08T11:36:08.296997Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4621893490d3415f9870784c4f65c6e3"}},"metadata":{}},{"name":"stdout","text":"{'amr_graph': '(z0 / write-01\\n    :ARG1 (z1 / article)\\n    :time (z2 / date-entity\\n              :day 18\\n              :month 12))', 'text': 'Translate from Graph to Text- Instruction: (z0 / write-01\\n    :ARG1 (z1 / article)\\n    :time (z2 / date-entity\\n              :day 18\\n              :month 12))\\n Response:The article was written on December 18th.', 'prompt': 'Translate from Graph to Text- Instruction: (z0 / write-01\\n    :ARG1 (z1 / article)\\n    :time (z2 / date-entity\\n              :day 18\\n              :month 12))\\n Response:', 'answer': 'The article was written on December 18th.'}\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, DataCollatorForSeq2Seq\nfrom typing import Dict, List\nfrom functools import partial\nimport copy\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\nfrom datasets import DatasetDict\n\n\nmodel_checkpoint = \"t5-small\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)\n\n# Set the EOS token as the padding token\ntokenizer.pad_token = tokenizer.eos_token\n\nMAX_LENGTH = 256\n\n# Function to generate token embeddings from the text part of the batch\ndef _preprocess_batch(batch: Dict[str, List]):  \n    model_inputs = tokenizer(batch[\"text\"], max_length=MAX_LENGTH, truncation=True, padding='max_length')    \n    model_inputs[\"labels\"] = copy.deepcopy(model_inputs['input_ids'])\n    return model_inputs\n\n_preprocessing_function = partial(_preprocess_batch)\n\n\n\n# Define the split ratios\ntrain_test_split = small_dataset.train_test_split(test_size=0.2)  # Split off 20% as test set\ntrain_valid_split = train_test_split['train'].train_test_split(test_size=0.1)  # From train, split 10% as validation\n\n# Combine splits into a DatasetDict\ndataset_dict = DatasetDict({\n    'train': train_valid_split['train'],\n    'validation': train_valid_split['test'],\n    'test': train_test_split['test']\n})\n\n# Print the size of each split to verify\nprint(f\"Train set size: {len(dataset_dict['train'])}\")\nprint(f\"Validation set size: {len(dataset_dict['validation'])}\")\nprint(f\"Test set size: {len(dataset_dict['test'])}\")\n\n# Example check for first item in each split\nprint(\"Sample from train:\", dataset_dict['train'][0])\nprint(\"Sample from validation:\", dataset_dict['validation'][0])\nprint(\"Sample from test:\", dataset_dict['test'][0])\n\n\n# Apply the preprocessing function to each batch in the dataset\nencoded_train_dataset = dataset_dict['train'].map(\n    _preprocessing_function,\n    batched=True,\n    remove_columns=[\"amr_graph\", \"text\", \"prompt\", \"answer\"],\n)\n\nencoded_validation_dataset = dataset_dict['validation'].map(\n    _preprocessing_function,\n    batched=True,\n    remove_columns=[\"amr_graph\", \"text\", \"prompt\", \"answer\"],\n)\n\nencoded_test_dataset = dataset_dict['test'].map(\n    _preprocessing_function,\n    batched=True,\n    remove_columns=[\"amr_graph\", \"text\", \"prompt\", \"answer\"],\n)\nprocessed_train_dataset = encoded_train_dataset.filter(lambda rec: len(rec[\"input_ids\"]) <= MAX_LENGTH)\nprocessed_validation_dataset = encoded_validation_dataset.filter(lambda rec: len(rec[\"input_ids\"]) <= MAX_LENGTH)\nprocessed_test_dataset = encoded_test_dataset.filter(lambda rec: len(rec[\"input_ids\"]) <= MAX_LENGTH)\n\n\n\n\n\n\n\n# print(processed_train_dataset)\n# print(processed_validation_dataset)\n# print(processed_test_dataset)\n","metadata":{"execution":{"iopub.status.busy":"2024-12-08T11:39:51.866593Z","iopub.execute_input":"2024-12-08T11:39:51.866966Z","iopub.status.idle":"2024-12-08T11:40:13.546211Z","shell.execute_reply.started":"2024-12-08T11:39:51.866938Z","shell.execute_reply":"2024-12-08T11:40:13.545411Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2afbf2769634f08a4187941971f698c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fd0a34930e241849b0ee8a6a29788d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c71661798b1e4530a11a0774e95608df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ad13b45d7d74642ba14461111547de7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c68f6ea518144ff9301b7b30723fcf9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb77fa8be62a41eea1462bced356dda1"}},"metadata":{}},{"name":"stdout","text":"Train set size: 18000\nValidation set size: 2000\nTest set size: 5000\nSample from train: {'amr_graph': '(z0 / like-01\\n    :ARG0 (z1 / person\\n              :name (z2 / name\\n                        :op1 \"Scott\"\\n                        :op2 \"Lighty\"))\\n    :ARG1 (z3 / car))', 'text': 'Translate from Graph to Text- Instruction: (z0 / like-01\\n    :ARG0 (z1 / person\\n              :name (z2 / name\\n                        :op1 \"Scott\"\\n                        :op2 \"Lighty\"))\\n    :ARG1 (z3 / car))\\n Response:Scott Lighty likes cars.', 'prompt': 'Translate from Graph to Text- Instruction: (z0 / like-01\\n    :ARG0 (z1 / person\\n              :name (z2 / name\\n                        :op1 \"Scott\"\\n                        :op2 \"Lighty\"))\\n    :ARG1 (z3 / car))\\n Response:', 'answer': 'Scott Lighty likes cars.'}\nSample from validation: {'amr_graph': '(z0 / have-degree-91\\n    :ARG1 (z1 / get-01\\n              :ARG1 (z2 / plant\\n                        :ARG1-of (z3 / root-02\\n                                     :time (z4 / before)))\\n              :source (z5 / forest))\\n    :ARG2 (z6 / good-02\\n              :ARG1 z1)\\n    :ARG3 (z7 / most))', 'text': 'Translate from Graph to Text- Instruction: (z0 / have-degree-91\\n    :ARG1 (z1 / get-01\\n              :ARG1 (z2 / plant\\n                        :ARG1-of (z3 / root-02\\n                                     :time (z4 / before)))\\n              :source (z5 / forest))\\n    :ARG2 (z6 / good-02\\n              :ARG1 z1)\\n    :ARG3 (z7 / most))\\n Response:It is best to get pre rooted plants from the forest.', 'prompt': 'Translate from Graph to Text- Instruction: (z0 / have-degree-91\\n    :ARG1 (z1 / get-01\\n              :ARG1 (z2 / plant\\n                        :ARG1-of (z3 / root-02\\n                                     :time (z4 / before)))\\n              :source (z5 / forest))\\n    :ARG2 (z6 / good-02\\n              :ARG1 z1)\\n    :ARG3 (z7 / most))\\n Response:', 'answer': 'It is best to get pre rooted plants from the forest.'}\nSample from test: {'amr_graph': '(z0 / mention-01\\n    :ARG1 (z1 / country\\n              :quant (z2 / over\\n                         :op1 2)))', 'text': 'Translate from Graph to Text- Instruction: (z0 / mention-01\\n    :ARG1 (z1 / country\\n              :quant (z2 / over\\n                         :op1 2)))\\n Response:Over 2 countries are mentioned', 'prompt': 'Translate from Graph to Text- Instruction: (z0 / mention-01\\n    :ARG1 (z1 / country\\n              :quant (z2 / over\\n                         :op1 2)))\\n Response:', 'answer': 'Over 2 countries are mentioned'}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/18000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30867efc10424bf483ac07462ce6d328"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e53abcab5a12481eaae2a06203aacf12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a3ef915841449738cd035f8f9f1b706"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/18000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62486c904e87409ba102381b8f19a9ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ec8f748ec3e4d969897dd6c86d50430"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20f8f60f56e246f98c7ea60f5976726a"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"train_valid_split['train']","metadata":{"execution":{"iopub.status.busy":"2024-12-08T11:40:17.542794Z","iopub.execute_input":"2024-12-08T11:40:17.543452Z","iopub.status.idle":"2024-12-08T11:40:17.549522Z","shell.execute_reply.started":"2024-12-08T11:40:17.543406Z","shell.execute_reply":"2024-12-08T11:40:17.548655Z"},"trusted":true},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['amr_graph', 'text', 'prompt', 'answer'],\n    num_rows: 18000\n})"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\nfrom datasets import load_dataset\nfrom transformers import DataCollatorForSeq2Seq\n\n# Enable W&B dry run mode\nos.environ[\"WANDB_MODE\"] = \"dryrun\"\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load the tokenizer and model for T5\nmodel_checkpoint = \"t5-small\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='/mnt/disks/disk1/results',\n    evaluation_strategy='epoch',\n    num_train_epochs=5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=4,  # Accumulate gradients for 4 steps\n    warmup_steps=50,\n    learning_rate=5e-3,        # Lowered learning rate\n    weight_decay=0.01,          # Reduced weight decay to prevent over-penalizing weights\n    logging_dir='/mnt/disks/disk1/logs'\n)\n\n# Initialize the data collator for sequence-to-sequence tasks\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n\n# Initialize Trainer with the data collator\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=processed_train_dataset,\n    eval_dataset=processed_validation_dataset,\n    data_collator=data_collator\n)\n\n# Train the model\ntrainer.train()\n\n# Save the model and tokenizer explicitly\nmodel_output_dir = '/mnt/disks/disk1/results'\nmodel.save_pretrained(model_output_dir)\ntokenizer.save_pretrained(model_output_dir)\n","metadata":{"execution":{"iopub.status.busy":"2024-12-08T11:48:10.406212Z","iopub.execute_input":"2024-12-08T11:48:10.406581Z","iopub.status.idle":"2024-12-08T12:27:26.005303Z","shell.execute_reply.started":"2024-12-08T11:48:10.406550Z","shell.execute_reply":"2024-12-08T12:27:26.004491Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5625' max='5625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5625/5625 39:13, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.008100</td>\n      <td>0.001092</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.004000</td>\n      <td>0.000594</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.001700</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.000600</td>\n      <td>0.000064</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.000300</td>\n      <td>0.000026</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"('/mnt/disks/disk1/results/tokenizer_config.json',\n '/mnt/disks/disk1/results/special_tokens_map.json',\n '/mnt/disks/disk1/results/spiece.model',\n '/mnt/disks/disk1/results/added_tokens.json',\n '/mnt/disks/disk1/results/tokenizer.json')"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"model.save_pretrained(model_output_dir)\ntokenizer.save_pretrained(model_output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T12:28:03.542067Z","iopub.execute_input":"2024-12-08T12:28:03.542418Z","iopub.status.idle":"2024-12-08T12:28:04.371910Z","shell.execute_reply.started":"2024-12-08T12:28:03.542384Z","shell.execute_reply":"2024-12-08T12:28:04.371033Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"('/mnt/disks/disk1/results/tokenizer_config.json',\n '/mnt/disks/disk1/results/special_tokens_map.json',\n '/mnt/disks/disk1/results/spiece.model',\n '/mnt/disks/disk1/results/added_tokens.json',\n '/mnt/disks/disk1/results/tokenizer.json')"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\n\ndef get_model_parameters(model):\n    total_params = sum(p.numel() for p in model.parameters())\n    return total_params\n\ndef main(input_text):\n    # Load the tokenizer and model from the saved directory\n    model_path = '/mnt/disks/disk1/results'\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n\n    # Calculate the number of parameters in the model being used for inference\n    total_params = get_model_parameters(model)\n    print(f\"Total number of parameters: {total_params}\")\n\n    # Prepare the input text for generation\n    inputs = tokenizer(input_text, return_tensors='pt')\n\n    # Generate text\n    outputs = model.generate(**inputs, max_length=100, num_return_sequences=1)\n\n    # Decode the generated text\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(\"Generated text:\", generated_text)\n\n# Example input for inference\nexample_input = \"(z0 / dilligent\\n:domain (z1 / doctor\\n:name (z2 / name\\n:op1 'Henry'\\n :op2 'Friesen')))\"\nmain(example_input)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T12:28:06.933718Z","iopub.execute_input":"2024-12-08T12:28:06.934066Z","iopub.status.idle":"2024-12-08T12:28:08.082389Z","shell.execute_reply.started":"2024-12-08T12:28:06.934036Z","shell.execute_reply":"2024-12-08T12:28:08.081501Z"}},"outputs":[{"name":"stdout","text":"Total number of parameters: 60506624\nGenerated text: Translate from :domain (z1 / doctor :name (z2 / name :op1 'Henry' :op2 'Friesen')))\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"print(dataset_dict['train'][10]['amr_graph'])\nprint(\"________________________________________\")\nprint(dataset_dict['train'][10]['answer'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T12:28:17.973140Z","iopub.execute_input":"2024-12-08T12:28:17.973501Z","iopub.status.idle":"2024-12-08T12:28:17.980835Z","shell.execute_reply.started":"2024-12-08T12:28:17.973464Z","shell.execute_reply":"2024-12-08T12:28:17.980127Z"}},"outputs":[{"name":"stdout","text":"(z0 / contain-01\n    :ARG0 (z1 / state\n              :name (z2 / name\n                        :op1 \"Texas\"))\n    :ARG1 (z3 / person\n              :ARG1-of (z4 / name-01\n                           :ARG2 (z5 / dean))))\n________________________________________\nTexas contains a person named dean\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport re\n\ndef get_model_parameters(model):\n    total_params = sum(p.numel() for p in model.parameters())\n    return total_params\n\ndef main(input_text):\n    # Load the tokenizer and model from the saved directory\n    model_path = '/mnt/disks/disk1/results'\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n\n    # Calculate the number of parameters in the model being used for inference\n    total_params = get_model_parameters(model)\n    #print(f\"Total number of parameters: {total_params}\")\n\n    # Prepare the input text for generation\n    inputs = tokenizer(input_text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n\n    # Generate text\n    outputs = model.generate(**inputs, max_length=500, num_return_sequences=1)\n\n    # Decode the generated text\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    # print(generated_text)\n    # Extract only the response part by splitting based on \"Response:\"\n    match = re.search(r\"Response:\\s*(.*)\", generated_text)\n    # print(generated_text)\n    if match:\n        response_text = match.group(1)\n        # Remove extra spaces between sentences\n        response_text = re.sub(r'\\s{2,}', ' ', response_text)\n        # Keep only up to the first sensible sentence-ending punctuation\n        response_text = re.split(r'[.!?]', response_text)[0].strip() + '.'\n        #print(\"Response text:\", response_text)\n        return response_text\n    else:\n        #print(\"Response not found in generated text\")\n        return \"Response not found in generated text\"\n\n# Example input for inference\nexample_input = \"\"\"\ntranslate from Graph to Text: (z0 / group\n    :name (z1 / name\n              :op1 \"Demix\")\n    :ARG0-of (z2 / experiment-01\n                 :polarity -))\n\"\"\"\noutput = main(example_input)\nprint(output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T12:28:18.629507Z","iopub.execute_input":"2024-12-08T12:28:18.629839Z","iopub.status.idle":"2024-12-08T12:28:19.864199Z","shell.execute_reply.started":"2024-12-08T12:28:18.629810Z","shell.execute_reply":"2024-12-08T12:28:19.863287Z"}},"outputs":[{"name":"stdout","text":"Response not found in generated text\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\ndef calculate_bleu(predicted_text, ground_truth_text):\n    # Tokenize the texts into lists of words\n    reference = [ground_truth_text.split()]  # BLEU expects a list of references\n    hypothesis = predicted_text.split()\n    \n    # Calculate BLEU score with smoothing\n    smoothie = SmoothingFunction().method4  # Use smoothing to handle short texts\n    bleu_score = sentence_bleu(reference, hypothesis, smoothing_function=smoothie)\n    \n    return bleu_score\n\n# Example texts\npredicted_text = \"The two balls went behind 8 balls\"\nground_truth_text = \"The 2-ball goes directly behind the 8-ball\"\n\n# Calculate and print BLEU score\nbleu = calculate_bleu(predicted_text, ground_truth_text)\nprint(f\"BLEU score: {bleu}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:36:08.365406Z","iopub.status.idle":"2024-12-08T11:36:08.365712Z","shell.execute_reply.started":"2024-12-08T11:36:08.365578Z","shell.execute_reply":"2024-12-08T11:36:08.365592Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Average bleu score calculations","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Initialize variables for BLEU score calculation\nbleu_score = 0\nvalid_count = 0  # Counter for valid BLEU scores\nk = 10  # Define the number of samples to evaluate\n\n# Lists to store generated and ground truth text, and AMR graphs for valid entries\ngenerated_texts = []\nground_truth_texts = []\nvalid_amr_graphs = []\n\n# List to store indices where BLEU score is invalid or input length > 500\ninvalid_indices = []\n\n# Loop through the dataset\nfor i in range(k):\n    example_input = dataset_dict['test'][i]['amr_graph']\n    ground_truth_text = dataset_dict['test'][i]['answer']\n    \n    # Tokenize and check input length\n    tokenized_input = tokenizer(example_input, return_tensors='pt')\n    input_length = tokenized_input['input_ids'].shape[1]\n    \n    # Skip examples with input length greater than 500\n    if input_length > 500:\n        invalid_indices.append(i)  # Store the index where input length is greater than 500\n        continue\n\n    # Generate model output and calculate BLEU score\n    model_output_text = main(example_input)\n    bleu = calculate_bleu(model_output_text, ground_truth_text)\n\n    # Store generated and ground truth text only if BLEU score is valid\n    if bleu > 0:\n        bleu_score += bleu\n        valid_count += 1  # Increment count of valid scores\n        generated_texts.append(model_output_text)\n        ground_truth_texts.append(ground_truth_text)\n        valid_amr_graphs.append(example_input)  # Include AMR graph for valid entries\n    else:\n        invalid_indices.append(i)  # Store the index where BLEU score is invalid (<= 0)\n\n# Calculate the average BLEU score only if there are valid scores\nif valid_count > 0:\n    avg_bleu_score = bleu_score / valid_count\nelse:\n    avg_bleu_score = 0.0  # Set average to zero if no valid scores were found\n\n# Print the average BLEU score and invalid indices\nprint(\"Average BLEU score:\", avg_bleu_score)\nprint(\"Invalid indices (input length > 500 or BLEU score <= 0):\", invalid_indices)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:36:08.367036Z","iopub.status.idle":"2024-12-08T11:36:08.367340Z","shell.execute_reply.started":"2024-12-08T11:36:08.367196Z","shell.execute_reply":"2024-12-08T11:36:08.367212Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Create a DataFrame to store the generated, ground truth text, and AMR graphs for valid entries\ndf = pd.DataFrame({\n    'Generated Text': generated_texts,\n    'Ground Truth Text': ground_truth_texts,\n    'AMR Graph': valid_amr_graphs  # Include the AMR graph only for valid entries\n})\n\n# Optionally, save to a CSV file\ndf.to_csv('generated_vs_ground_truth_with_amr.csv', index=False)\n\n# Print the first few rows of the DataFrame\nprint(df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:36:08.368583Z","iopub.status.idle":"2024-12-08T11:36:08.369004Z","shell.execute_reply.started":"2024-12-08T11:36:08.368787Z","shell.execute_reply":"2024-12-08T11:36:08.368809Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['AMR Graph']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:36:08.370163Z","iopub.status.idle":"2024-12-08T11:36:08.370625Z","shell.execute_reply.started":"2024-12-08T11:36:08.370368Z","shell.execute_reply":"2024-12-08T11:36:08.370389Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Finetuned Model","metadata":{}},{"cell_type":"code","source":"import re\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n# Path to the fine-tuned model directory\nmodel_path = \"/kaggle/input/graph2sequencefinetune-model/checkpoint-3375\"\n\n# Load the tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")  # Assuming standard GPT-2 tokenizer; change if custom\n\n# Load the model\nmodel = GPT2LMHeadModel.from_pretrained(\n    model_path,  # Path to the directory containing the checkpoint\n    local_files_only=True  # Load files from the local path\n)\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# Text for inference\ninput_text = \"\"\"\n(z0 / go-01\n    :ARG1 (z1 / ball\n              :quant 2)\n    :ARG1-of (z2 / direct-02)\n    :ARG4 (z3 / behind\n              :op1 (z4 / ball\n                       :quant 8)))\n\"\"\"\n\n# Tokenize the input\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n\n# Generate predictions\noutputs = model.generate(\n    input_ids,\n    max_length=256,  # Set the maximum length of the generated text\n    num_beams=5,    # Beam search for better quality\n    no_repeat_ngram_size=2,  # Avoid repetitive n-grams\n    early_stopping=True\n)\n\n# Decode the generated text\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Extract only the response part\nmatch = re.search(r\"Response:\\s*(.*)\", generated_text)\nif match:\n    response_text = match.group(1)\n    # Remove extra spaces between sentences\n    response_text = re.sub(r'\\s{2,}', ' ', response_text)\n    # Keep only up to the first sensible sentence-ending punctuation\n    response_text = re.split(r'[.!?]', response_text)[0].strip() + '.'\n    print(\"Generated text:\", response_text)\nelse:\n    print(\"Response not found in generated text.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T11:36:08.372567Z","iopub.status.idle":"2024-12-08T11:36:08.372863Z","shell.execute_reply.started":"2024-12-08T11:36:08.372723Z","shell.execute_reply":"2024-12-08T11:36:08.372737Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}