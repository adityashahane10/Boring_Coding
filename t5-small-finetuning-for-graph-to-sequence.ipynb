{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9435733,"sourceType":"datasetVersion","datasetId":5733239},{"sourceId":9435879,"sourceType":"datasetVersion","datasetId":5733337},{"sourceId":10130014,"sourceType":"datasetVersion","datasetId":6251681}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets\n!pip install penman\n!wget -O amr3.0.tar.gz https://amr.isi.edu/download/amr-bank-3.0.txt\n!mkdir amr_data\n!tar -xvzf amr3.0.tar.gz -C amr_data\n!pip install transformers datasets torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-12-12T04:41:32.202967Z","iopub.execute_input":"2024-12-12T04:41:32.203853Z","iopub.status.idle":"2024-12-12T04:42:00.368923Z","shell.execute_reply.started":"2024-12-12T04:41:32.203817Z","shell.execute_reply":"2024-12-12T04:42:00.368091Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.1.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.26.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.6.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: penman in /opt/conda/lib/python3.10/site-packages (1.3.1)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"--2024-12-12 04:41:49--  https://amr.isi.edu/download/amr-bank-3.0.txt\nResolving amr.isi.edu (amr.isi.edu)... failed: Name or service not known.\nwget: unable to resolve host address 'amr.isi.edu'\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"mkdir: cannot create directory 'amr_data': File exists\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\ngzip: stdin: unexpected end of file\ntar: Child returned status 1\ntar: Error is not recoverable: exiting now\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.1.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.26.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}],"execution_count":68},{"cell_type":"markdown","source":"### AMR dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load AMR dataset\ndataset = load_dataset(\"tverous/anli-amr\", split=\"train\")\n\n# View the data\nprint(dataset[0])\nprint(dataset.shape)","metadata":{"execution":{"iopub.status.busy":"2024-12-12T04:42:00.370831Z","iopub.execute_input":"2024-12-12T04:42:00.371114Z","iopub.status.idle":"2024-12-12T04:42:02.494819Z","shell.execute_reply.started":"2024-12-12T04:42:00.371087Z","shell.execute_reply":"2024-12-12T04:42:02.493924Z"},"trusted":true},"outputs":[{"name":"stdout","text":"{'uid': '2093cfb3-a15f-4282-81e3-0cb793ffd0d7', 'premise': 'TOKYO, Dec 18 (Reuters) - Japan’s Shionogi & Co said on Tuesday that it has applied to health regulators in the United States, Canada and Europe for approval of its HIV drug Dolutegravir. Shionogi developed Dolutegravir with a Viiv Healthcare, an AIDS drug joint venture between GlaxoSmithKline and Pfizer, in exchange for its rights to the drug.', 'hypothesis': 'The article was written on December 18th.', 'label': 0, 'reason': 'TOKYO, Dec 18 (Reuters) is when the article was written as it states in the first words of the sentence', 'claim_cleaned_amr': '( z0 write :ARG1 ( z1 article ) :time ( z2 date-entity :day 18 :month 12 ) )', 'amr_penman': '(z0 / write-01\\n    :ARG1 (z1 / article)\\n    :time (z2 / date-entity\\n              :day 18\\n              :month 12))', 'amr_tokens': ['The', 'article', 'was', 'written', 'on', 'December', '18th', '.'], 'amr_nodes': \"{'z1': 'article', 'z0': 'write-01', 'z2': 'date-entity', '0': '12', '1': '18'}\", 'amr_alignments': \"{'z1': [1], 'z0': [3], 'z2': [5], '0': [5], '1': [6]}\", 'amr_edges': [['z0', ':ARG1', 'z1'], ['z0', ':time', 'z2'], ['z2', ':month', '0'], ['z2', ':day', '1']]}\n(100459, 11)\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"from datasets import load_dataset\nimport pandas as pd\n\n\n\n# Function to extract AMR graph and text\ndef extract_amr_and_text(data):\n    amr_text_pairs = []\n    for row in data:\n        amr_graph = row.get(\"amr_penman\", None)\n        text = row.get(\"hypothesis\", None)\n        if amr_graph and text:\n            amr_text_pairs.append({\"amr_graph\": amr_graph, \"text\": text})\n    return amr_text_pairs\n\n# Extract AMR graphs and texts for all rows\namr_text_pairs = extract_amr_and_text(dataset)\n\namrs = []\ntexts = []\n\nfor i in range(100459):\n    amrs.append(amr_text_pairs[i]['amr_graph'])\n    texts.append(amr_text_pairs[i]['text'])\n\n# Creating DataFrame with 'amr_graph' and 'text' columns\ndata_amr = pd.DataFrame({\n    'amr_graph': amrs,\n    'text': texts\n})\n\ndata_amr.head()  # Displaying the first few rows to verify","metadata":{"execution":{"iopub.status.busy":"2024-12-12T04:42:02.495935Z","iopub.execute_input":"2024-12-12T04:42:02.496343Z","iopub.status.idle":"2024-12-12T04:42:15.034068Z","shell.execute_reply.started":"2024-12-12T04:42:02.496288Z","shell.execute_reply":"2024-12-12T04:42:15.033223Z"},"trusted":true},"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"                                           amr_graph  \\\n0  (z0 / write-01\\n    :ARG1 (z1 / article)\\n    ...   \n1  (z0 / urge-01\\n    :ARG0 (z1 / person\\n       ...   \n2  (z0 / and\\n    :op1 (z1 / beat-03\\n           ...   \n3  (z0 / close-01\\n    :ARG1 (z1 / road)\\n    :du...   \n4    (z0 / advise-01\\n    :ARG2 (z1 / slow-down-03))   \n\n                                                text  \n0          The article was written on December 18th.  \n1  Gillum was on TV urging residents to stay out ...  \n2  Carlton beat Melbourne in 2016 and will attemp...  \n3  The road was closed for more than two hours af...  \n4                         Its advisible to slow down  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>amr_graph</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(z0 / write-01\\n    :ARG1 (z1 / article)\\n    ...</td>\n      <td>The article was written on December 18th.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(z0 / urge-01\\n    :ARG0 (z1 / person\\n       ...</td>\n      <td>Gillum was on TV urging residents to stay out ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>(z0 / and\\n    :op1 (z1 / beat-03\\n           ...</td>\n      <td>Carlton beat Melbourne in 2016 and will attemp...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>(z0 / close-01\\n    :ARG1 (z1 / road)\\n    :du...</td>\n      <td>The road was closed for more than two hours af...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(z0 / advise-01\\n    :ARG2 (z1 / slow-down-03))</td>\n      <td>Its advisible to slow down</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":70},{"cell_type":"markdown","source":"### Transformer Finetuning","metadata":{}},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Convert the DataFrame to a Hugging Face dataset\ndataset = Dataset.from_pandas(data_amr)\n\n# Select a smaller subset if needed\nsmall_dataset = dataset.select([i for i in range(25000)])\n\n# Define prompt and answer templates\nprompt_template = \"\"\"Translate from Graph to Text- Instruction: {instruction}\\n \"\"\"\nanswer_template = \"\"\"Response: {response}\"\"\"\n\n# Define function to add keys in the dictionary for prompt, answer, and combined text\ndef _add_text(rec):\n    instruction = rec[\"amr_graph\"]  # Use amr_graph as instruction\n    response = rec[\"text\"]  # Use text as response\n    \n    # Check if both exist; raise error if not\n    if not instruction:\n        raise ValueError(f\"Expected an instruction (amr_graph) in: {rec}\")\n    if not response:\n        raise ValueError(f\"Expected a response (text) in: {rec}\")\n    \n    # Create prompt, answer, and combined text\n    rec[\"prompt\"] = prompt_template.format(instruction=instruction)\n    rec[\"answer\"] = answer_template.format(response=response)\n    rec[\"text\"] = rec[\"prompt\"] + rec[\"answer\"]\n    return rec\n\n# Apply the function to the dataset\nsmall_dataset = small_dataset.map(_add_text)\n\n# Print the first item to check\nprint(small_dataset[0])\n","metadata":{"execution":{"iopub.status.busy":"2024-12-12T04:42:15.035756Z","iopub.execute_input":"2024-12-12T04:42:15.036048Z","iopub.status.idle":"2024-12-12T04:42:16.631097Z","shell.execute_reply.started":"2024-12-12T04:42:15.036022Z","shell.execute_reply":"2024-12-12T04:42:16.630246Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f61ab98de85458089166223610da4e1"}},"metadata":{}},{"name":"stdout","text":"{'amr_graph': '(z0 / write-01\\n    :ARG1 (z1 / article)\\n    :time (z2 / date-entity\\n              :day 18\\n              :month 12))', 'text': 'Translate from Graph to Text- Instruction: (z0 / write-01\\n    :ARG1 (z1 / article)\\n    :time (z2 / date-entity\\n              :day 18\\n              :month 12))\\n Response: The article was written on December 18th.', 'prompt': 'Translate from Graph to Text- Instruction: (z0 / write-01\\n    :ARG1 (z1 / article)\\n    :time (z2 / date-entity\\n              :day 18\\n              :month 12))\\n ', 'answer': 'Response: The article was written on December 18th.'}\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, DataCollatorForSeq2Seq\nfrom typing import Dict, List\nfrom functools import partial\nimport copy\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\nfrom datasets import DatasetDict\n\n\nmodel_checkpoint = \"t5-small\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)\n\n# Set the EOS token as the padding token\ntokenizer.pad_token = tokenizer.eos_token\n\nMAX_LENGTH = 256\n\n# Function to generate token embeddings from the text part of the batch\n# def _preprocess_batch(batch: Dict[str, List]):  \n#     model_inputs = tokenizer(batch[\"text\"], max_length=MAX_LENGTH, truncation=True, padding='max_length')    \n#     model_inputs[\"labels\"] = copy.deepcopy(model_inputs['input_ids'])\n#     return model_inputs\n\ndef _preprocess_batch(batch: Dict[str, List]):  \n    model_inputs = tokenizer(batch[\"prompt\"], max_length=MAX_LENGTH, truncation=True, padding='max_length')  \n    targets = tokenizer(batch[\"answer\"], max_length=MAX_LENGTH, truncation=True, padding='max_length')  \n    model_inputs[\"labels\"] = targets[\"input_ids\"]\n    return model_inputs\n\n_preprocessing_function = partial(_preprocess_batch)\n\n\n\n# Define the split ratios\ntrain_test_split = small_dataset.train_test_split(test_size=0.2)  # Split off 20% as test set\ntrain_valid_split = train_test_split['train'].train_test_split(test_size=0.1)  # From train, split 10% as validation\n\n# Combine splits into a DatasetDict\ndataset_dict = DatasetDict({\n    'train': train_valid_split['train'],\n    'validation': train_valid_split['test'],\n    'test': train_test_split['test']\n})\n\n# Print the size of each split to verify\nprint(f\"Train set size: {len(dataset_dict['train'])}\")\nprint(f\"Validation set size: {len(dataset_dict['validation'])}\")\nprint(f\"Test set size: {len(dataset_dict['test'])}\")\n\n# Example check for first item in each split\nprint(\"Sample from train:\", dataset_dict['train'][0])\nprint(\"Sample from validation:\", dataset_dict['validation'][0])\nprint(\"Sample from test:\", dataset_dict['test'][0])\n\n\n# Apply the preprocessing function to each batch in the dataset\nencoded_train_dataset = dataset_dict['train'].map(\n    _preprocessing_function,\n    batched=True,\n    remove_columns=[\"amr_graph\", \"text\", \"prompt\", \"answer\"],\n)\n\nencoded_validation_dataset = dataset_dict['validation'].map(\n    _preprocessing_function,\n    batched=True,\n    remove_columns=[\"amr_graph\", \"text\", \"prompt\", \"answer\"],\n)\n\nencoded_test_dataset = dataset_dict['test'].map(\n    _preprocessing_function,\n    batched=True,\n    remove_columns=[\"amr_graph\", \"text\", \"prompt\", \"answer\"],\n)\nprocessed_train_dataset = encoded_train_dataset.filter(lambda rec: len(rec[\"input_ids\"]) <= MAX_LENGTH)\nprocessed_validation_dataset = encoded_validation_dataset.filter(lambda rec: len(rec[\"input_ids\"]) <= MAX_LENGTH)\nprocessed_test_dataset = encoded_test_dataset.filter(lambda rec: len(rec[\"input_ids\"]) <= MAX_LENGTH)\n\n\n\n\n\n\n\n# print(processed_train_dataset)\n# print(processed_validation_dataset)\n# print(processed_test_dataset)\n","metadata":{"execution":{"iopub.status.busy":"2024-12-12T04:42:16.632595Z","iopub.execute_input":"2024-12-12T04:42:16.633116Z","iopub.status.idle":"2024-12-12T04:42:31.190807Z","shell.execute_reply.started":"2024-12-12T04:42:16.633075Z","shell.execute_reply":"2024-12-12T04:42:31.190066Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Train set size: 18000\nValidation set size: 2000\nTest set size: 5000\nSample from train: {'amr_graph': '(z0 / sophomore\\n    :domain (z1 / girl\\n                :topic-of (z2 / article))\\n    :location (z3 / college))', 'text': 'Translate from Graph to Text- Instruction: (z0 / sophomore\\n    :domain (z1 / girl\\n                :topic-of (z2 / article))\\n    :location (z3 / college))\\n Response: The girl in the article is a sophomore in college ', 'prompt': 'Translate from Graph to Text- Instruction: (z0 / sophomore\\n    :domain (z1 / girl\\n                :topic-of (z2 / article))\\n    :location (z3 / college))\\n ', 'answer': 'Response: The girl in the article is a sophomore in college '}\nSample from validation: {'amr_graph': '(z0 / keep-02\\n    :ARG0 (z1 / person\\n              :name (z2 / name\\n                        :op1 \"Tom\"))\\n    :ARG1 (z3 / talk-01\\n              :ARG0 z1\\n              :ARG2 (z4 / she))\\n    :time (z5 / after\\n              :op1 (z6 / move-01\\n                       :ARG0 z1)))', 'text': 'Translate from Graph to Text- Instruction: (z0 / keep-02\\n    :ARG0 (z1 / person\\n              :name (z2 / name\\n                        :op1 \"Tom\"))\\n    :ARG1 (z3 / talk-01\\n              :ARG0 z1\\n              :ARG2 (z4 / she))\\n    :time (z5 / after\\n              :op1 (z6 / move-01\\n                       :ARG0 z1)))\\n Response: Tom kept talking to her after he moved', 'prompt': 'Translate from Graph to Text- Instruction: (z0 / keep-02\\n    :ARG0 (z1 / person\\n              :name (z2 / name\\n                        :op1 \"Tom\"))\\n    :ARG1 (z3 / talk-01\\n              :ARG0 z1\\n              :ARG2 (z4 / she))\\n    :time (z5 / after\\n              :op1 (z6 / move-01\\n                       :ARG0 z1)))\\n ', 'answer': 'Response: Tom kept talking to her after he moved'}\nSample from test: {'amr_graph': '(z0 / say-01\\n    :ARG0 (z1 / memory\\n              :poss (z2 / person\\n                        :mod (z3 / this)))\\n    :ARG1 (z4 / subpoena-01\\n              :ARG1 (z5 / person\\n                        :ARG0-of (z6 / report-01))\\n              :ARG2 (z7 / testify-01\\n                        :ARG0 z5)\\n              :time (z8 / before\\n                        :quant (z9 / temporal-quantity\\n                                   :quant 3\\n                                   :unit (z10 / day))\\n                        :op1 (z11 / now))))', 'text': \"Translate from Graph to Text- Instruction: (z0 / say-01\\n    :ARG0 (z1 / memory\\n              :poss (z2 / person\\n                        :mod (z3 / this)))\\n    :ARG1 (z4 / subpoena-01\\n              :ARG1 (z5 / person\\n                        :ARG0-of (z6 / report-01))\\n              :ARG2 (z7 / testify-01\\n                        :ARG0 z5)\\n              :time (z8 / before\\n                        :quant (z9 / temporal-quantity\\n                                   :quant 3\\n                                   :unit (z10 / day))\\n                        :op1 (z11 / now))))\\n Response: The reporters were subpoenaed to testify 3 days ago, according to this person's memory.\", 'prompt': 'Translate from Graph to Text- Instruction: (z0 / say-01\\n    :ARG0 (z1 / memory\\n              :poss (z2 / person\\n                        :mod (z3 / this)))\\n    :ARG1 (z4 / subpoena-01\\n              :ARG1 (z5 / person\\n                        :ARG0-of (z6 / report-01))\\n              :ARG2 (z7 / testify-01\\n                        :ARG0 z5)\\n              :time (z8 / before\\n                        :quant (z9 / temporal-quantity\\n                                   :quant 3\\n                                   :unit (z10 / day))\\n                        :op1 (z11 / now))))\\n ', 'answer': \"Response: The reporters were subpoenaed to testify 3 days ago, according to this person's memory.\"}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/18000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7b5c5befe1147db9949a20f4c5e48c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"113032e42b714095b996fd358d104fc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c93619425ade46aba69b93d4ce13ea95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/18000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c64a0bb411b4d7586d51f3c626648f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16cf656ed66c473295bff524090c7839"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"749929ead1574a0ab9815d02f6e70f45"}},"metadata":{}}],"execution_count":72},{"cell_type":"code","source":"# break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T04:42:31.191802Z","iopub.execute_input":"2024-12-12T04:42:31.192053Z","iopub.status.idle":"2024-12-12T04:42:31.196697Z","shell.execute_reply.started":"2024-12-12T04:42:31.192028Z","shell.execute_reply":"2024-12-12T04:42:31.195782Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"train_valid_split['train']","metadata":{"execution":{"iopub.status.busy":"2024-12-12T04:42:31.197582Z","iopub.execute_input":"2024-12-12T04:42:31.197873Z","iopub.status.idle":"2024-12-12T04:42:31.208663Z","shell.execute_reply.started":"2024-12-12T04:42:31.197848Z","shell.execute_reply":"2024-12-12T04:42:31.207856Z"},"trusted":true},"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['amr_graph', 'text', 'prompt', 'answer'],\n    num_rows: 18000\n})"},"metadata":{}}],"execution_count":74},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\nfrom datasets import load_dataset\nfrom transformers import DataCollatorForSeq2Seq\n\n# Enable W&B dry run mode\nos.environ[\"WANDB_MODE\"] = \"dryrun\"\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load the tokenizer and model for T5\nmodel_checkpoint = \"t5-small\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='/mnt/disks/disk1/results',\n    evaluation_strategy='epoch',\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=4,  # Accumulate gradients for 4 steps\n    warmup_steps=50,\n    learning_rate=5e-4,        # Lowered learning rate\n    weight_decay=0.01,          # Reduced weight decay to prevent over-penalizing weights\n    logging_dir='/mnt/disks/disk1/logs'\n)\n\n# Initialize the data collator for sequence-to-sequence tasks\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n\n# Initialize Trainer with the data collator\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=processed_train_dataset,\n    eval_dataset=processed_validation_dataset,\n    data_collator=data_collator\n)\n\n# Train the model\ntrainer.train()\n\n# Save the model and tokenizer explicitly\nmodel_output_dir = '/mnt/disks/disk1/results'\nmodel.save_pretrained(model_output_dir)\ntokenizer.save_pretrained(model_output_dir)\n","metadata":{"execution":{"iopub.status.busy":"2024-12-12T04:42:31.209705Z","iopub.execute_input":"2024-12-12T04:42:31.210050Z","iopub.status.idle":"2024-12-12T06:13:31.842046Z","shell.execute_reply.started":"2024-12-12T04:42:31.210010Z","shell.execute_reply":"2024-12-12T06:13:31.841241Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1be51ece5a474fb29dcf2e8c99793a4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"874364a68cb1482aadf026a11d6dc21f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa6925f622454154978b54e0684df658"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24ca8a76375b4e8ab959b1bde3acd62d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e6585b646fc4e32a6a672bd7c88b8f8"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6750' max='6750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6750/6750 1:30:48, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.059700</td>\n      <td>0.051246</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.038000</td>\n      <td>0.048181</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.026100</td>\n      <td>0.049326</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"('/mnt/disks/disk1/results/tokenizer_config.json',\n '/mnt/disks/disk1/results/special_tokens_map.json',\n '/mnt/disks/disk1/results/spiece.model',\n '/mnt/disks/disk1/results/added_tokens.json',\n '/mnt/disks/disk1/results/tokenizer.json')"},"metadata":{}}],"execution_count":75},{"cell_type":"code","source":"# model.save_pretrained(model_output_dir)\n# tokenizer.save_pretrained(model_output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T06:13:31.843066Z","iopub.execute_input":"2024-12-12T06:13:31.843352Z","iopub.status.idle":"2024-12-12T06:13:35.370156Z","shell.execute_reply.started":"2024-12-12T06:13:31.843325Z","shell.execute_reply":"2024-12-12T06:13:35.369106Z"}},"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"('/mnt/disks/disk1/results/tokenizer_config.json',\n '/mnt/disks/disk1/results/special_tokens_map.json',\n '/mnt/disks/disk1/results/spiece.model',\n '/mnt/disks/disk1/results/added_tokens.json',\n '/mnt/disks/disk1/results/tokenizer.json')"},"metadata":{}}],"execution_count":76},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\n\ndef get_model_parameters(model):\n    total_params = sum(p.numel() for p in model.parameters())\n    return total_params\n\ndef main(input_text):\n    # Load the tokenizer and model from the saved directory\n    model_path = '/mnt/disks/disk1/results'\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n\n    # Calculate the number of parameters in the model being used for inference\n    total_params = get_model_parameters(model)\n    print(f\"Total number of parameters: {total_params}\")\n\n    # Prepare the input text for generation\n    inputs = tokenizer(input_text, return_tensors='pt')\n\n    # Generate text\n    outputs = model.generate(**inputs, max_length=100, num_return_sequences=1)\n\n    # Decode the generated text\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(\"Generated text:\", generated_text)\n\n# Example input for inference\nexample_input = \"(z0 / dilligent\\n:domain (z1 / doctor\\n:name (z2 / name\\n:op1 'Henry'\\n :op2 'Friesen')))\"\nmain(example_input)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T06:13:35.373974Z","iopub.execute_input":"2024-12-12T06:13:35.374257Z","iopub.status.idle":"2024-12-12T06:13:40.121095Z","shell.execute_reply.started":"2024-12-12T06:13:35.374230Z","shell.execute_reply":"2024-12-12T06:13:40.120125Z"}},"outputs":[{"name":"stdout","text":"Total number of parameters: 222903552\nGenerated text: Response: Dr. Henry Friesen was dilligent\n","output_type":"stream"}],"execution_count":77},{"cell_type":"code","source":"print(dataset_dict['train'][10]['amr_graph'])\nprint(\"________________________________________\")\nprint(dataset_dict['train'][10]['answer'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T06:13:40.122210Z","iopub.execute_input":"2024-12-12T06:13:40.122498Z","iopub.status.idle":"2024-12-12T06:13:40.129868Z","shell.execute_reply.started":"2024-12-12T06:13:40.122472Z","shell.execute_reply":"2024-12-12T06:13:40.128853Z"}},"outputs":[{"name":"stdout","text":"(z0 / contain-01\n    :ARG0 (z1 / school)\n    :ARG1 (z2 / string-entity\n              :value \"xx\"))\n________________________________________\nResponse: school contains a xx\n","output_type":"stream"}],"execution_count":78},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport re\n\ndef get_model_parameters(model):\n    total_params = sum(p.numel() for p in model.parameters())\n    return total_params\n\ndef main(input_text):\n    # Load the tokenizer and model from the saved directory\n    model_path = '/mnt/disks/disk1/results'\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n\n    # Calculate the number of parameters in the model being used for inference\n    total_params = get_model_parameters(model)\n    #print(f\"Total number of parameters: {total_params}\")\n\n    # Prepare the input text for generation\n    inputs = tokenizer(input_text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n\n    # Generate text\n    outputs = model.generate(**inputs, max_length=500, num_return_sequences=1)\n\n    # Decode the generated text\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    # print(generated_text)\n    # Extract only the response part by splitting based on \"Response:\"\n    match = re.search(r\"Response:\\s*(.*)\", generated_text)\n    # print(generated_text)\n    if match:\n        response_text = match.group(1)\n        # Remove extra spaces between sentences\n        response_text = re.sub(r'\\s{2,}', ' ', response_text)\n        # Keep only up to the first sensible sentence-ending punctuation\n        response_text = re.split(r'[.!?]', response_text)[0].strip() + '.'\n        #print(\"Response text:\", response_text)\n        return response_text\n    else:\n        #print(\"Response not found in generated text\")\n        return \"Response not found in generated text\"\n\n# Example input for inference\nexample_input = \"\"\"\ntranslate from Graph to Text: (z0 / kill-01\n    :ARG0 (z1 / tsunami)\n    :ARG1 (z2 / person\n              :quant (z3 / more-than\n                         :op1 10)))\n\"\"\"\noutput = main(example_input)\nprint(output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T06:23:14.805118Z","iopub.execute_input":"2024-12-12T06:23:14.805476Z","iopub.status.idle":"2024-12-12T06:23:15.876901Z","shell.execute_reply.started":"2024-12-12T06:23:14.805446Z","shell.execute_reply":"2024-12-12T06:23:15.875881Z"}},"outputs":[{"name":"stdout","text":"The tsunami killed more than 10 people.\n","output_type":"stream"}],"execution_count":89},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\ndef calculate_bleu(predicted_text, ground_truth_text):\n    # Tokenize the texts into lists of words\n    reference = [ground_truth_text.split()]  # BLEU expects a list of references\n    hypothesis = predicted_text.split()\n    \n    # Calculate BLEU score with smoothing\n    smoothie = SmoothingFunction().method4  # Use smoothing to handle short texts\n    bleu_score = sentence_bleu(reference, hypothesis, smoothing_function=smoothie)\n    \n    return bleu_score\n\n# Example texts\npredicted_text = \"The tsunami killed more than 10 people.\"\nground_truth_text = \"The tsunami killed more than 10 people.\"\n\n# Calculate and print BLEU score\nbleu = calculate_bleu(predicted_text, ground_truth_text)\nprint(f\"BLEU score: {bleu}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T06:13:41.269359Z","iopub.execute_input":"2024-12-12T06:13:41.269658Z","iopub.status.idle":"2024-12-12T06:13:41.277819Z","shell.execute_reply.started":"2024-12-12T06:13:41.269632Z","shell.execute_reply":"2024-12-12T06:13:41.276968Z"}},"outputs":[{"name":"stdout","text":"BLEU score: 1.0\n","output_type":"stream"}],"execution_count":80},{"cell_type":"markdown","source":"## Average bleu score calculations","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Initialize variables for BLEU score calculation\nbleu_score = 0\nvalid_count = 0  # Counter for valid BLEU scores\nk = 100  # Define the number of samples to evaluate\n\n# Lists to store generated and ground truth text, and AMR graphs for valid entries\ngenerated_texts = []\nground_truth_texts = []\nvalid_amr_graphs = []\n\n# List to store indices where BLEU score is invalid or input length > 500\ninvalid_indices = []\n\n# Loop through the dataset\nfor i in range(k):\n    example_input = dataset_dict['test'][i]['amr_graph']\n    # Append the additional string to the example_input\n    example_input = f\"Translate to Graph to Sequence: {example_input}\"\n    \n    # print(example_input)\n    ground_truth_text = dataset_dict['test'][i]['answer']\n    \n    # Tokenize and check input length\n    tokenized_input = tokenizer(example_input, return_tensors='pt')\n    input_length = tokenized_input['input_ids'].shape[1]\n    \n    # Skip examples with input length greater than 500\n    if input_length > 500:\n        invalid_indices.append(i)  # Store the index where input length is greater than 500\n        continue\n\n    # Generate model output and calculate BLEU score\n    model_output_text = main(example_input)\n    match = re.search(r\"Response:\\s*(.*)\", model_output_text)\n    # print(generated_text)\n    if match:\n        response_text = match.group(1)\n        # Remove extra spaces between sentences\n        response_text = re.sub(r'\\s{2,}', ' ', response_text)\n        # Keep only up to the first sensible sentence-ending punctuation\n        response_text = re.split(r'[.!?]', response_text)[0].strip() + '.'\n        print(\"Response text:\", response_text)\n        \n        bleu = calculate_bleu(response_text, ground_truth_text)\n        \n    # Store generated and ground truth text only if BLEU score is valid\n    if bleu > 0:\n        bleu_score += bleu\n        valid_count += 1  # Increment count of valid scores\n        # generated_texts.append(model_output_text)\n        generated_texts.append(model_output_text)\n        ground_truth_texts.append(ground_truth_text)\n        valid_amr_graphs.append(example_input)  # Include AMR graph for valid entries\n    else:\n        invalid_indices.append(i)  # Store the index where BLEU score is invalid (<= 0)\n\n# Calculate the average BLEU score only if there are valid scores\nif valid_count > 0:\n    avg_bleu_score = bleu_score / valid_count\nelse:\n    avg_bleu_score = 0.0  # Set average to zero if no valid scores were found\n\n# Print the average BLEU score and invalid indices\nprint(\"Average BLEU score:\", avg_bleu_score)\nprint(\"Invalid indices (input length > 500 or BLEU score <= 0):\", invalid_indices)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T06:14:26.584111Z","iopub.execute_input":"2024-12-12T06:14:26.584734Z","iopub.status.idle":"2024-12-12T06:18:16.235645Z","shell.execute_reply.started":"2024-12-12T06:14:26.584698Z","shell.execute_reply":"2024-12-12T06:18:16.234742Z"}},"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (1261 > 1024). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"Average BLEU score: 1.0\nInvalid indices (input length > 500 or BLEU score <= 0): [26, 36, 41, 50, 67, 94, 98, 128, 139, 141, 143, 165, 170]\n","output_type":"stream"}],"execution_count":85},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T04:31:08.366817Z","iopub.execute_input":"2024-12-12T04:31:08.367161Z","iopub.status.idle":"2024-12-12T04:31:08.373636Z","shell.execute_reply.started":"2024-12-12T04:31:08.367129Z","shell.execute_reply":"2024-12-12T04:31:08.372734Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"['Response not found in generated text',\n 'Response not found in generated text',\n 'Response not found in generated text']"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"\n# Create a DataFrame to store the generated, ground truth text, and AMR graphs for valid entries\ndf = pd.DataFrame({\n    'Generated Text': generated_texts,\n    'Ground Truth Text': ground_truth_texts,\n    'AMR Graph': valid_amr_graphs  # Include the AMR graph only for valid entries\n})\n\n# Optionally, save to a CSV file\ndf.to_csv('generated_vs_ground_truth_with_amr.csv', index=False)\n\n# Print the first few rows of the DataFrame\nprint(df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T06:18:16.237094Z","iopub.execute_input":"2024-12-12T06:18:16.237446Z","iopub.status.idle":"2024-12-12T06:18:16.250706Z","shell.execute_reply.started":"2024-12-12T06:18:16.237418Z","shell.execute_reply":"2024-12-12T06:18:16.249610Z"}},"outputs":[{"name":"stdout","text":"                                      Generated Text  \\\n0  The reporters were subpoenaed to testify three...   \n1                 Morton Halperin's home was tapped.   \n2     Pelopia became mother and sister the same day.   \n3                           Brown fields are rising.   \n4                             Maryland is published.   \n\n                                   Ground Truth Text  \\\n0  Response: The reporters were subpoenaed to tes...   \n1  Response:  Morton Halperin's home was wiretapped.   \n2  Response: Pelopia became a mother and a sister...   \n3             Response: Brown fields are on the rise   \n4  Response: The Maryland lotteries had been publ...   \n\n                                           AMR Graph  \n0  Translate to Graph to Sequence: (z0 / say-01\\n...  \n1  Translate to Graph to Sequence: (z0 / tap-03\\n...  \n2  Translate to Graph to Sequence: (z0 / become-0...  \n3  Translate to Graph to Sequence: (z0 / rise-01\\...  \n4  Translate to Graph to Sequence: (z0 / publish-...  \n","output_type":"stream"}],"execution_count":86},{"cell_type":"code","source":"df['AMR Graph']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T06:18:36.142052Z","iopub.execute_input":"2024-12-12T06:18:36.142947Z","iopub.status.idle":"2024-12-12T06:18:36.151803Z","shell.execute_reply.started":"2024-12-12T06:18:36.142909Z","shell.execute_reply":"2024-12-12T06:18:36.151011Z"}},"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"0      Translate to Graph to Sequence: (z0 / say-01\\n...\n1      Translate to Graph to Sequence: (z0 / tap-03\\n...\n2      Translate to Graph to Sequence: (z0 / become-0...\n3      Translate to Graph to Sequence: (z0 / rise-01\\...\n4      Translate to Graph to Sequence: (z0 / publish-...\n                             ...                        \n182    Translate to Graph to Sequence: (z0 / techniqu...\n183    Translate to Graph to Sequence: (z0 / have-03\\...\n184    Translate to Graph to Sequence: (z0 / visit-01...\n185    Translate to Graph to Sequence: (z0 / direct-0...\n186    Translate to Graph to Sequence: (z0 / be-locat...\nName: AMR Graph, Length: 187, dtype: object"},"metadata":{}}],"execution_count":87},{"cell_type":"markdown","source":"### Finetuned Model","metadata":{}},{"cell_type":"code","source":"import re\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n# Path to the fine-tuned model directory\nmodel_path = \"/kaggle/input/graph2sequencefinetune-model/checkpoint-3375\"\n\n# Load the tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")  # Assuming standard GPT-2 tokenizer; change if custom\n\n# Load the model\nmodel = GPT2LMHeadModel.from_pretrained(\n    model_path,  # Path to the directory containing the checkpoint\n    local_files_only=True  # Load files from the local path\n)\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# Text for inference\ninput_text = \"\"\"\n(z0 / go-01\n    :ARG1 (z1 / ball\n              :quant 2)\n    :ARG1-of (z2 / direct-02)\n    :ARG4 (z3 / behind\n              :op1 (z4 / ball\n                       :quant 8)))\n\"\"\"\n\n# Tokenize the input\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n\n# Generate predictions\noutputs = model.generate(\n    input_ids,\n    max_length=256,  # Set the maximum length of the generated text\n    num_beams=5,    # Beam search for better quality\n    no_repeat_ngram_size=2,  # Avoid repetitive n-grams\n    early_stopping=True\n)\n\n# Decode the generated text\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Extract only the response part\nmatch = re.search(r\"Response:\\s*(.*)\", generated_text)\nif match:\n    response_text = match.group(1)\n    # Remove extra spaces between sentences\n    response_text = re.sub(r'\\s{2,}', ' ', response_text)\n    # Keep only up to the first sensible sentence-ending punctuation\n    response_text = re.split(r'[.!?]', response_text)[0].strip() + '.'\n    print(\"Generated text:\", response_text)\nelse:\n    print(\"Response not found in generated text.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T06:13:48.325734Z","iopub.execute_input":"2024-12-12T06:13:48.326045Z","iopub.status.idle":"2024-12-12T06:14:08.708603Z","shell.execute_reply.started":"2024-12-12T06:13:48.326010Z","shell.execute_reply":"2024-12-12T06:14:08.707850Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1350018b764f4303b5d64ea499705926"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4be62c2653d14b1eb77d3b2c4dc1ece8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3ea215ecc81489eaf737ef9416841ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db1a4453618b4b14aff3d47ab3eff47b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d751ec6378b34ccfa37d89c8837a7bf0"}},"metadata":{}},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"Generated text: The two balls were directed directly behind the 8-ball.\n","output_type":"stream"}],"execution_count":84},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}