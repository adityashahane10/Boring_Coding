{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9435733,"sourceType":"datasetVersion","datasetId":5733239},{"sourceId":9435879,"sourceType":"datasetVersion","datasetId":5733337},{"sourceId":10130014,"sourceType":"datasetVersion","datasetId":6251681}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets\n!pip install penman\n!wget -O amr3.0.tar.gz https://amr.isi.edu/download/amr-bank-3.0.txt\n!mkdir amr_data\n!tar -xvzf amr3.0.tar.gz -C amr_data\n!pip install transformers datasets torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-12-02T08:57:36.733459Z","iopub.execute_input":"2024-12-02T08:57:36.734388Z","iopub.status.idle":"2024-12-02T08:58:07.871270Z","shell.execute_reply.started":"2024-12-02T08:57:36.734334Z","shell.execute_reply":"2024-12-02T08:58:07.870368Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nCollecting penman\n  Downloading penman-1.3.1-py3-none-any.whl.metadata (7.7 kB)\nDownloading penman-1.3.1-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.4/43.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: penman\nSuccessfully installed penman-1.3.1\n--2024-12-02 08:57:56--  https://amr.isi.edu/download/amr-bank-3.0.txt\nResolving amr.isi.edu (amr.isi.edu)... failed: Name or service not known.\nwget: unable to resolve host address 'amr.isi.edu'\n\ngzip: stdin: unexpected end of file\ntar: Child returned status 1\ntar: Error is not recoverable: exiting now\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### AMR dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load AMR dataset\ndataset = load_dataset(\"tverous/anli-amr\", split=\"train\")\n\n# View the data\nprint(dataset[0])\nprint(dataset.shape)","metadata":{"execution":{"iopub.status.busy":"2024-12-02T08:58:07.873425Z","iopub.execute_input":"2024-12-02T08:58:07.873896Z","iopub.status.idle":"2024-12-02T08:58:14.286603Z","shell.execute_reply.started":"2024-12-02T08:58:07.873854Z","shell.execute_reply":"2024-12-02T08:58:14.285746Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/991 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"beecf324b771465b883d621e5456b939"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(‚Ä¶)-00000-of-00001-99aac9afd7e04376.parquet:   0%|          | 0.00/42.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34fddcd09067417e8f56a7ee6dd0ea68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(‚Ä¶)-00000-of-00001-a639eb6d952016f3.parquet:   0%|          | 0.00/847k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8ef67404b324a76865d7f742260fc7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(‚Ä¶)-00000-of-00001-9eb6c125c93e351c.parquet:   0%|          | 0.00/851k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d5140cfc8dc44149c38dd998815a8bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/100459 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b431d025098c48bbbef9a526d2acad9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33afe4e187ab46dc97f385df76c531b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating dev split:   0%|          | 0/1200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3dc9088668a4d229903297c47e593e5"}},"metadata":{}},{"name":"stdout","text":"{'uid': '2093cfb3-a15f-4282-81e3-0cb793ffd0d7', 'premise': 'TOKYO, Dec 18 (Reuters) - Japan‚Äôs Shionogi & Co said on Tuesday that it has applied to health regulators in the United States, Canada and Europe for approval of its HIV drug Dolutegravir. Shionogi developed Dolutegravir with a Viiv Healthcare, an AIDS drug joint venture between GlaxoSmithKline and Pfizer, in exchange for its rights to the drug.', 'hypothesis': 'The article was written on December 18th.', 'label': 0, 'reason': 'TOKYO, Dec 18 (Reuters) is when the article was written as it states in the first words of the sentence', 'claim_cleaned_amr': '( z0 write :ARG1 ( z1 article ) :time ( z2 date-entity :day 18 :month 12 ) )', 'amr_penman': '(z0 / write-01\\n    :ARG1 (z1 / article)\\n    :time (z2 / date-entity\\n              :day 18\\n              :month 12))', 'amr_tokens': ['The', 'article', 'was', 'written', 'on', 'December', '18th', '.'], 'amr_nodes': \"{'z1': 'article', 'z0': 'write-01', 'z2': 'date-entity', '0': '12', '1': '18'}\", 'amr_alignments': \"{'z1': [1], 'z0': [3], 'z2': [5], '0': [5], '1': [6]}\", 'amr_edges': [['z0', ':ARG1', 'z1'], ['z0', ':time', 'z2'], ['z2', ':month', '0'], ['z2', ':day', '1']]}\n(100459, 11)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from datasets import load_dataset\nimport pandas as pd\n\n\n\n# Function to extract AMR graph and text\ndef extract_amr_and_text(data):\n    amr_text_pairs = []\n    for row in data:\n        amr_graph = row.get(\"amr_penman\", None)\n        text = row.get(\"hypothesis\", None)\n        if amr_graph and text:\n            amr_text_pairs.append({\"amr_graph\": amr_graph, \"text\": text})\n    return amr_text_pairs\n\n# Extract AMR graphs and texts for all rows\namr_text_pairs = extract_amr_and_text(dataset)\n\namrs = []\ntexts = []\n\nfor i in range(100459):\n    amrs.append(amr_text_pairs[i]['amr_graph'])\n    texts.append(amr_text_pairs[i]['text'])\n\n# Creating DataFrame with 'amr_graph' and 'text' columns\ndata_amr = pd.DataFrame({\n    'amr_graph': amrs,\n    'text': texts\n})\n\ndata_amr.head()  # Displaying the first few rows to verify","metadata":{"execution":{"iopub.status.busy":"2024-12-02T08:58:14.287808Z","iopub.execute_input":"2024-12-02T08:58:14.288446Z","iopub.status.idle":"2024-12-02T08:58:26.428589Z","shell.execute_reply.started":"2024-12-02T08:58:14.288405Z","shell.execute_reply":"2024-12-02T08:58:26.427634Z"},"trusted":true},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                           amr_graph  \\\n0  (z0 / write-01\\n    :ARG1 (z1 / article)\\n    ...   \n1  (z0 / urge-01\\n    :ARG0 (z1 / person\\n       ...   \n2  (z0 / and\\n    :op1 (z1 / beat-03\\n           ...   \n3  (z0 / close-01\\n    :ARG1 (z1 / road)\\n    :du...   \n4    (z0 / advise-01\\n    :ARG2 (z1 / slow-down-03))   \n\n                                                text  \n0          The article was written on December 18th.  \n1  Gillum was on TV urging residents to stay out ...  \n2  Carlton beat Melbourne in 2016 and will attemp...  \n3  The road was closed for more than two hours af...  \n4                         Its advisible to slow down  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>amr_graph</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(z0 / write-01\\n    :ARG1 (z1 / article)\\n    ...</td>\n      <td>The article was written on December 18th.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(z0 / urge-01\\n    :ARG0 (z1 / person\\n       ...</td>\n      <td>Gillum was on TV urging residents to stay out ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>(z0 / and\\n    :op1 (z1 / beat-03\\n           ...</td>\n      <td>Carlton beat Melbourne in 2016 and will attemp...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>(z0 / close-01\\n    :ARG1 (z1 / road)\\n    :du...</td>\n      <td>The road was closed for more than two hours af...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(z0 / advise-01\\n    :ARG2 (z1 / slow-down-03))</td>\n      <td>Its advisible to slow down</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"### Transformer Finetuning","metadata":{}},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Convert the DataFrame to a Hugging Face dataset\ndataset = Dataset.from_pandas(data_amr)\n\n# Select a smaller subset if needed\nsmall_dataset = dataset.select([i for i in range(25000)])\n\n# Define prompt and answer templates\nprompt_template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request. Instruction: {instruction}\\n Response:\"\"\"\nanswer_template = \"\"\"{response}\"\"\"\n\n# Define function to add keys in the dictionary for prompt, answer, and combined text\ndef _add_text(rec):\n    instruction = rec[\"amr_graph\"]  # Use amr_graph as instruction\n    response = rec[\"text\"]  # Use text as response\n    \n    # Check if both exist; raise error if not\n    if not instruction:\n        raise ValueError(f\"Expected an instruction (amr_graph) in: {rec}\")\n    if not response:\n        raise ValueError(f\"Expected a response (text) in: {rec}\")\n    \n    # Create prompt, answer, and combined text\n    rec[\"prompt\"] = prompt_template.format(instruction=instruction)\n    rec[\"answer\"] = answer_template.format(response=response)\n    rec[\"text\"] = rec[\"prompt\"] + rec[\"answer\"]\n    return rec\n\n# Apply the function to the dataset\nsmall_dataset = small_dataset.map(_add_text)\n\n# Print the first item to check\nprint(small_dataset[0])\n","metadata":{"execution":{"iopub.status.busy":"2024-12-02T08:58:26.431070Z","iopub.execute_input":"2024-12-02T08:58:26.431758Z","iopub.status.idle":"2024-12-02T08:58:35.400776Z","shell.execute_reply.started":"2024-12-02T08:58:26.431715Z","shell.execute_reply":"2024-12-02T08:58:35.399693Z"},"trusted":true},"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"965296af51c240a0986d3a309154a85f"}},"metadata":{}},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a2313af1d384af785da90a7a8c7eaa2"}},"metadata":{}},{"name":"stdout","text":"{'amr_graph': '(z0 / write-01\\n    :ARG1 (z1 / article)\\n    :time (z2 / date-entity\\n              :day 18\\n              :month 12))', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request. Instruction: (z0 / write-01\\n    :ARG1 (z1 / article)\\n    :time (z2 / date-entity\\n              :day 18\\n              :month 12))\\n Response:The article was written on December 18th.', 'prompt': 'Below is an instruction that describes a task. Write a response that appropriately completes the request. Instruction: (z0 / write-01\\n    :ARG1 (z1 / article)\\n    :time (z2 / date-entity\\n              :day 18\\n              :month 12))\\n Response:', 'answer': 'The article was written on December 18th.'}\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, DataCollatorForSeq2Seq\nfrom typing import Dict, List\nfrom functools import partial\nimport copy\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\nfrom datasets import DatasetDict\n\n# Load tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained('gpt2')\nmodel = AutoModelForCausalLM.from_pretrained('gpt2').to(device)\n\n# Set the EOS token as the padding token\ntokenizer.pad_token = tokenizer.eos_token\n\nMAX_LENGTH = 256\n\n# Function to generate token embeddings from the text part of the batch\ndef _preprocess_batch(batch: Dict[str, List]):  \n    model_inputs = tokenizer(batch[\"text\"], max_length=MAX_LENGTH, truncation=True, padding='max_length')    \n    model_inputs[\"labels\"] = copy.deepcopy(model_inputs['input_ids'])\n    return model_inputs\n\n_preprocessing_function = partial(_preprocess_batch)\n\n\n\n# Define the split ratios\ntrain_test_split = small_dataset.train_test_split(test_size=0.2)  # Split off 20% as test set\ntrain_valid_split = train_test_split['train'].train_test_split(test_size=0.1)  # From train, split 10% as validation\n\n# Combine splits into a DatasetDict\ndataset_dict = DatasetDict({\n    'train': train_valid_split['train'],\n    'validation': train_valid_split['test'],\n    'test': train_test_split['test']\n})\n\n# Print the size of each split to verify\nprint(f\"Train set size: {len(dataset_dict['train'])}\")\nprint(f\"Validation set size: {len(dataset_dict['validation'])}\")\nprint(f\"Test set size: {len(dataset_dict['test'])}\")\n\n# Example check for first item in each split\nprint(\"Sample from train:\", dataset_dict['train'][0])\nprint(\"Sample from validation:\", dataset_dict['validation'][0])\nprint(\"Sample from test:\", dataset_dict['test'][0])\n\n\n# Apply the preprocessing function to each batch in the dataset\nencoded_train_dataset = dataset_dict['train'].map(\n    _preprocessing_function,\n    batched=True,\n    remove_columns=[\"amr_graph\", \"text\", \"prompt\", \"answer\"],\n)\n\nencoded_validation_dataset = dataset_dict['validation'].map(\n    _preprocessing_function,\n    batched=True,\n    remove_columns=[\"amr_graph\", \"text\", \"prompt\", \"answer\"],\n)\n\nencoded_test_dataset = dataset_dict['test'].map(\n    _preprocessing_function,\n    batched=True,\n    remove_columns=[\"amr_graph\", \"text\", \"prompt\", \"answer\"],\n)\nprocessed_train_dataset = encoded_train_dataset.filter(lambda rec: len(rec[\"input_ids\"]) <= MAX_LENGTH)\nprocessed_validation_dataset = encoded_validation_dataset.filter(lambda rec: len(rec[\"input_ids\"]) <= MAX_LENGTH)\nprocessed_test_dataset = encoded_test_dataset.filter(lambda rec: len(rec[\"input_ids\"]) <= MAX_LENGTH)\n\n\n\n\n\n\n\nprint(processed_train_dataset)\nprint(processed_validation_dataset)\nprint(processed_test_dataset)\n","metadata":{"execution":{"iopub.status.busy":"2024-12-02T08:58:35.402244Z","iopub.execute_input":"2024-12-02T08:58:35.402636Z","iopub.status.idle":"2024-12-02T08:59:17.534133Z","shell.execute_reply.started":"2024-12-02T08:58:35.402595Z","shell.execute_reply":"2024-12-02T08:59:17.533152Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"094c24f4b5024947a42a11fa7a1318a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ef04aa442714bd9848dfe786a0a9a74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc2201bde10d41e296a946cb2f0acdfd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41f8ca4fb8174a0aa83cdc5de11a4109"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0145463815d24287ba4dcddb00356e6c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5587dbd6c3f4c53a9a3c71a128bfad1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2681b449860f400e93f032cd87e0d2f4"}},"metadata":{}},{"name":"stdout","text":"Train set size: 18000\nValidation set size: 2000\nTest set size: 5000\nSample from train: {'amr_graph': '(z0 / eat-01\\n    :ARG0 (z1 / we)\\n    :ARG1 (z2 / pizza\\n              :ARG1-of (z3 / cook-01\\n                           :location (z4 / oven))))', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request. Instruction: (z0 / eat-01\\n    :ARG0 (z1 / we)\\n    :ARG1 (z2 / pizza\\n              :ARG1-of (z3 / cook-01\\n                           :location (z4 / oven))))\\n Response:We ate the pizza that was cooked in the oven.', 'prompt': 'Below is an instruction that describes a task. Write a response that appropriately completes the request. Instruction: (z0 / eat-01\\n    :ARG0 (z1 / we)\\n    :ARG1 (z2 / pizza\\n              :ARG1-of (z3 / cook-01\\n                           :location (z4 / oven))))\\n Response:', 'answer': 'We ate the pizza that was cooked in the oven.'}\nSample from validation: {'amr_graph': '(z0 / scour-02\\n    :ARG1 (z1 / person\\n              :name (z2 / name\\n                        :op1 \"Chao\"\\n                        :op2 \"Moo-hyun\"))\\n    :ARG2 (z3 / company\\n              :name (z4 / name\\n                        :op1 \"Ice\"\\n                        :op2 \"Creamusume\"))\\n    :location (z5 / street))', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request. Instruction: (z0 / scour-02\\n    :ARG1 (z1 / person\\n              :name (z2 / name\\n                        :op1 \"Chao\"\\n                        :op2 \"Moo-hyun\"))\\n    :ARG2 (z3 / company\\n              :name (z4 / name\\n                        :op1 \"Ice\"\\n                        :op2 \"Creamusume\"))\\n    :location (z5 / street))\\n Response:Chao Kuo-Jung was scouted for Ice Creamusume on the street', 'prompt': 'Below is an instruction that describes a task. Write a response that appropriately completes the request. Instruction: (z0 / scour-02\\n    :ARG1 (z1 / person\\n              :name (z2 / name\\n                        :op1 \"Chao\"\\n                        :op2 \"Moo-hyun\"))\\n    :ARG2 (z3 / company\\n              :name (z4 / name\\n                        :op1 \"Ice\"\\n                        :op2 \"Creamusume\"))\\n    :location (z5 / street))\\n Response:', 'answer': 'Chao Kuo-Jung was scouted for Ice Creamusume on the street'}\nSample from test: {'amr_graph': '(z0 / have-03\\n    :ARG0 (z1 / debt\\n              :mod (z2 / world\\n                       :ord (z3 / ordinal-entity\\n                                :value 3)))\\n    :ARG1 (z4 / zero))', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request. Instruction: (z0 / have-03\\n    :ARG0 (z1 / debt\\n              :mod (z2 / world\\n                       :ord (z3 / ordinal-entity\\n                                :value 3)))\\n    :ARG1 (z4 / zero))\\n Response:Third World debt has a zz', 'prompt': 'Below is an instruction that describes a task. Write a response that appropriately completes the request. Instruction: (z0 / have-03\\n    :ARG0 (z1 / debt\\n              :mod (z2 / world\\n                       :ord (z3 / ordinal-entity\\n                                :value 3)))\\n    :ARG1 (z4 / zero))\\n Response:', 'answer': 'Third World debt has a zz'}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/18000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e4ad4550161413b9350d0b1dd0ea90b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1c669218fc945e0bdfb92ba4e0c222b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3d063a7254f423b8990666fdb0da272"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/18000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e79e9160705e4106b7c83ab193f2121c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eddce5fe174042459c43b79a781f784d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee3125f6a5f444be8bda5bf8b0f7bf2c"}},"metadata":{}},{"name":"stdout","text":"Dataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 18000\n})\nDataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 2000\n})\nDataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 5000\n})\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"train_valid_split['train']","metadata":{"execution":{"iopub.status.busy":"2024-12-02T08:59:17.535327Z","iopub.execute_input":"2024-12-02T08:59:17.536076Z","iopub.status.idle":"2024-12-02T08:59:17.541856Z","shell.execute_reply.started":"2024-12-02T08:59:17.536034Z","shell.execute_reply":"2024-12-02T08:59:17.541052Z"},"trusted":true},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['amr_graph', 'text', 'prompt', 'answer'],\n    num_rows: 18000\n})"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\nfrom datasets import load_dataset\nfrom transformers import DataCollatorForLanguageModeling\n# Enable W&B dry run mode\nos.environ[\"WANDB_MODE\"] = \"dryrun\"\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\n\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='/mnt/disks/disk1/results',\n    evaluation_strategy='epoch',\n    num_train_epochs=3,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=4,  # Accumulate gradients for 4 steps\n    warmup_steps=50,\n    learning_rate=1e-4,        # Lowered learning rate\n    weight_decay=0.1,          # Reduced weight decay to prevent over-penalizing weights\n    logging_dir='/mnt/disks/disk1/logs'\n)\n\n\n\n\n# Initialize the data collator for causal language modeling\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n# Initialize Trainer with the data collator\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=processed_train_dataset,\n    eval_dataset=processed_validation_dataset,\n    data_collator=data_collator\n)\n\n\n\n# Train the model\ntrainer.train()\n\n# Save the model and tokenizer explicitly\nmodel_output_dir = '/mnt/disks/disk1/results'\nmodel.save_pretrained(model_output_dir)\ntokenizer.save_pretrained(model_output_dir)","metadata":{"execution":{"iopub.status.busy":"2024-12-02T08:59:17.542864Z","iopub.execute_input":"2024-12-02T08:59:17.543150Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='433' max='6750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 433/6750 05:32 < 1:21:17, 1.30 it/s, Epoch 0.19/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(model_output_dir)\ntokenizer.save_pretrained(model_output_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\n\ndef get_model_parameters(model):\n    total_params = sum(p.numel() for p in model.parameters())\n    return total_params\n\ndef main(input_text):\n    # Load the tokenizer and model from the saved directory\n    model_path = model_output_dir\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForCausalLM.from_pretrained(model_path)\n\n    # Calculate the number of parameters in the model being used for inference\n    total_params = get_model_parameters(model)\n    print(f\"Total number of parameters: {total_params}\")\n\n    # Prepare the input text for generation\n    inputs = tokenizer(input_text, return_tensors='pt')\n\n    # Generate text\n    outputs = model.generate(**inputs, max_length=100, num_return_sequences=1)\n\n    # Decode the generated text\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(\"Generated text:\", generated_text)\n\n# Example input for inference\nexample_input = \"(z0 / dilligent\\n:domain (z1 / doctor\\n:name (z2 / name\\n:op1 'Henry'\\n :op2 'Friesen')))\"\nmain(example_input)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(dataset_dict['test'][120]['amr_graph'])\nprint(\"________________________________________\")\nprint(dataset_dict['test'][120]['answer'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport re\ndef get_model_parameters(model):\n    total_params = sum(p.numel() for p in model.parameters())\n    return total_params\n\ndef main(input_text):\n    # Load the tokenizer and model from the saved directory\n    model_path = '/mnt/disks/disk1/results'\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForCausalLM.from_pretrained(model_path)\n\n    # Calculate the number of parameters in the model being used for inference\n    total_params = get_model_parameters(model)\n    \"\"\"\n    i have commented this print stetement to avoid any print in the average calculation:\n    \"\"\"\n    #print(f\"Total number of parameters: {total_params}\")\n\n    # Prepare the input text for generation\n    inputs = tokenizer(input_text, return_tensors='pt')\n\n    # Generate text\n    outputs = model.generate(**inputs, max_length=500, num_return_sequences=1)\n\n    # Decode the generated text\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    # Extract only the response part by splitting based on \"Response:\"\n\n    match = re.search(r\"Response:\\s*(.*)\", generated_text)\n    if match:\n        response_text = match.group(1)\n        # Remove extra spaces between sentences\n        response_text = re.sub(r'\\s{2,}', ' ', response_text)\n        # Keep only up to the first sensible sentence-ending punctuation\n        response_text = re.split(r'[.!?]', response_text)[0].strip() + '.'\n        #print(\"Response text:\", response_text)\n        return response_text\n    else:\n        #print(\"Response not found in generated text\")\n        return \"Response not found in generated text\"\n\n# Example input for inference\nexample_input = \"\"\"\n(z0 / easy-05\n    :ARG1 (z1 / scare-01\n              :ARG1 (z2 / person\n                        :ARG0-of (z3 / have-rel-role-91\n                                     :ARG1 (z4 / i)\n                                     :ARG2 (z5 / uncle))))\n    :mod (z6 / certain))\n\"\"\"\noutput = main(example_input)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\n# def calculate_bleu(predicted_text, ground_truth_text):\n#     # Tokenize the texts into lists of words\n#     reference = [ground_truth_text.split()]  # BLEU expects a list of references\n#     hypothesis = predicted_text.split()\n    \n#     # Calculate BLEU score with smoothing\n#     smoothie = SmoothingFunction().method4  # Use smoothing to handle short texts\n#     bleu_score = sentence_bleu(reference, hypothesis, smoothing_function=smoothie)\n    \n#     return bleu_score\n\n# # Example texts\n# predicted_text = \" My uncle was certain that I was scared\"\n# ground_truth_text = \"It was certainly easy to scare my uncle\"\n\n# # Calculate and print BLEU score\n# bleu = calculate_bleu(predicted_text, ground_truth_text)\n# print(f\"BLEU score: {bleu}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Average bleu score calculations","metadata":{}},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\ndef calculate_bleu(predicted_text, ground_truth_text):\n    # Tokenize the texts into lists of words\n    reference = [ground_truth_text.split()]  # BLEU expects a list of references\n    hypothesis = predicted_text.split()\n\n    # Return 0 BLEU score if the hypothesis is empty\n    if not hypothesis:\n        return 0.0\n    \n    # Calculate BLEU score with smoothing\n    smoothie = SmoothingFunction().method4  # Use smoothing to handle short texts\n    bleu_score = sentence_bleu(reference, hypothesis, smoothing_function=smoothie)\n\n    return bleu_score\n\n# Initialize variables for BLEU score calculation\nbleu_score = 0\nvalid_count = 0  # Counter for valid BLEU scores\nk = 10  # Define the number of samples to evaluate\n\n# Loop through the dataset\nfor i in range(k):\n    example_input = dataset_dict['test'][i]['amr_graph']\n    ground_truth_text = dataset_dict['test'][i]['answer']\n    \n    # Tokenize and check input length\n    tokenized_input = tokenizer(example_input, return_tensors='pt')\n    input_length = tokenized_input['input_ids'].shape[1]\n    \n    # Skip examples with input length greater than 500\n    if input_length > 500:\n        continue\n\n    # Generate model output and calculate BLEU score\n    model_output_text = main(example_input)\n    bleu = calculate_bleu(model_output_text, ground_truth_text)\n\n    # Only add BLEU score if it‚Äôs valid (greater than zero)\n    if bleu > 0:\n        bleu_score += bleu\n        valid_count += 1  # Increment count of valid scores\n\n# Calculate the average BLEU score only if there are valid scores\nif valid_count > 0:\n    avg_bleu_score = bleu_score / valid_count\nelse:\n    avg_bleu_score = 0.0  # Set average to zero if no valid scores were found\n\nprint(\"Average BLEU score:\", avg_bleu_score)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}